{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dangerous-kinase",
   "metadata": {},
   "source": [
    "# CAPEC co-occurrence matrix using phrases\n",
    "Ivan Ulloa, 2-14-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ongoing-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-promotion",
   "metadata": {},
   "source": [
    "##  Load CAPEC dataset and save to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fallen-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('data/1000.xml')\n",
    "xml_data = tree.getroot()\n",
    "# Change the encoding type to be able to set it to the one you need\n",
    "xmlstr = ET.tostring(xml_data, encoding='utf-8', method='xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "focal-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "capec = xmltodict.parse(xmlstr)\n",
    "with open('data/capec_data.json', 'w') as f:\n",
    "    f.write(json.dumps(capec))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "selected-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dict = dict(xmltodict.parse(xmlstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-average",
   "metadata": {},
   "source": [
    "## Extract ID, Descriptions, CAPEC relationships, and CWE Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "controlled-willow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ID = []\n",
    "Desc = []\n",
    "Rel_CAPEC = []\n",
    "Rel_CWE = []\n",
    "for i in range(len(capec['ns0:Attack_Pattern_Catalog']['ns0:Attack_Patterns']['ns0:Attack_Pattern'])):\n",
    "    ID.append(capec['ns0:Attack_Pattern_Catalog']['ns0:Attack_Patterns']['ns0:Attack_Pattern'][i]['@ID'])\n",
    "    Desc.append(capec['ns0:Attack_Pattern_Catalog']['ns0:Attack_Patterns']['ns0:Attack_Pattern'][i]['ns0:Description'])\n",
    "    try:\n",
    "        Rel_CAPEC.append(capec['ns0:Attack_Pattern_Catalog']['ns0:Attack_Patterns']['ns0:Attack_Pattern'][i]['ns0:Related_Attack_Patterns']['ns0:Related_Attack_Pattern'])\n",
    "    except:\n",
    "        Rel_CAPEC.append('None')\n",
    "    try:\n",
    "        Rel_CWE.append(capec['ns0:Attack_Pattern_Catalog']['ns0:Attack_Patterns']['ns0:Attack_Pattern'][i]['ns0:Related_Weaknesses']['ns0:Related_Weakness'])\n",
    "    except:\n",
    "        Rel_CWE.append('None')\n",
    "dict = {'ID': ID, 'Description': Desc, 'rel_CAPEC':Rel_CAPEC, 'rel_CWE':Rel_CWE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "civilian-rwanda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Description</th>\n",
       "      <th>rel_CAPEC</th>\n",
       "      <th>rel_CWE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>In applications, particularly web applications...</td>\n",
       "      <td>[{'@CAPEC_ID': '122', '@Nature': 'ChildOf'}, {...</td>\n",
       "      <td>[{'@CWE_ID': '276'}, {'@CWE_ID': '285'}, {'@CW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>This attack pattern involves causing a buffer ...</td>\n",
       "      <td>{'@CAPEC_ID': '100', '@Nature': 'ChildOf'}</td>\n",
       "      <td>[{'@CWE_ID': '120'}, {'@CWE_ID': '302'}, {'@CW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Buffer Overflow attacks target improper or mis...</td>\n",
       "      <td>{'@CAPEC_ID': '123', '@Nature': 'ChildOf'}</td>\n",
       "      <td>[{'@CWE_ID': '120'}, {'@CWE_ID': '119'}, {'@CW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>An attacker can use Server Side Include (SSI) ...</td>\n",
       "      <td>{'@CAPEC_ID': '253', '@Nature': 'ChildOf'}</td>\n",
       "      <td>[{'@CWE_ID': '97'}, {'@CWE_ID': '74'}, {'@CWE_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>Session sidejacking takes advantage of an unen...</td>\n",
       "      <td>{'@CAPEC_ID': '593', '@Nature': 'ChildOf'}</td>\n",
       "      <td>[{'@CWE_ID': '294'}, {'@CWE_ID': '522'}, {'@CW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>94</td>\n",
       "      <td>This type of attack targets the communication ...</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'@CWE_ID': '300'}, {'@CWE_ID': '290'}, {'@CW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>95</td>\n",
       "      <td>This attack targets the WSDL interface made av...</td>\n",
       "      <td>{'@CAPEC_ID': '54', '@Nature': 'ChildOf'}</td>\n",
       "      <td>{'@CWE_ID': '538'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>96</td>\n",
       "      <td>An application typically makes calls to functi...</td>\n",
       "      <td>{'@CAPEC_ID': '603', '@Nature': 'ChildOf', 'ns...</td>\n",
       "      <td>[{'@CWE_ID': '589'}, {'@CWE_ID': '227'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>97</td>\n",
       "      <td>Cryptanalysis is a process of finding weakness...</td>\n",
       "      <td>[{'@CAPEC_ID': '192', '@Nature': 'ChildOf'}, {...</td>\n",
       "      <td>[{'@CWE_ID': '327'}, {'@CWE_ID': '1240'}, {'@C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>98</td>\n",
       "      <td>Phishing is a social engineering technique whe...</td>\n",
       "      <td>[{'@CAPEC_ID': '151', '@Nature': 'ChildOf', 'n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>527 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                        Description  \\\n",
       "0      1  In applications, particularly web applications...   \n",
       "1     10  This attack pattern involves causing a buffer ...   \n",
       "2    100  Buffer Overflow attacks target improper or mis...   \n",
       "3    101  An attacker can use Server Side Include (SSI) ...   \n",
       "4    102  Session sidejacking takes advantage of an unen...   \n",
       "..   ...                                                ...   \n",
       "522   94  This type of attack targets the communication ...   \n",
       "523   95  This attack targets the WSDL interface made av...   \n",
       "524   96  An application typically makes calls to functi...   \n",
       "525   97  Cryptanalysis is a process of finding weakness...   \n",
       "526   98  Phishing is a social engineering technique whe...   \n",
       "\n",
       "                                             rel_CAPEC  \\\n",
       "0    [{'@CAPEC_ID': '122', '@Nature': 'ChildOf'}, {...   \n",
       "1           {'@CAPEC_ID': '100', '@Nature': 'ChildOf'}   \n",
       "2           {'@CAPEC_ID': '123', '@Nature': 'ChildOf'}   \n",
       "3           {'@CAPEC_ID': '253', '@Nature': 'ChildOf'}   \n",
       "4           {'@CAPEC_ID': '593', '@Nature': 'ChildOf'}   \n",
       "..                                                 ...   \n",
       "522                                               None   \n",
       "523          {'@CAPEC_ID': '54', '@Nature': 'ChildOf'}   \n",
       "524  {'@CAPEC_ID': '603', '@Nature': 'ChildOf', 'ns...   \n",
       "525  [{'@CAPEC_ID': '192', '@Nature': 'ChildOf'}, {...   \n",
       "526  [{'@CAPEC_ID': '151', '@Nature': 'ChildOf', 'n...   \n",
       "\n",
       "                                               rel_CWE  \n",
       "0    [{'@CWE_ID': '276'}, {'@CWE_ID': '285'}, {'@CW...  \n",
       "1    [{'@CWE_ID': '120'}, {'@CWE_ID': '302'}, {'@CW...  \n",
       "2    [{'@CWE_ID': '120'}, {'@CWE_ID': '119'}, {'@CW...  \n",
       "3    [{'@CWE_ID': '97'}, {'@CWE_ID': '74'}, {'@CWE_...  \n",
       "4    [{'@CWE_ID': '294'}, {'@CWE_ID': '522'}, {'@CW...  \n",
       "..                                                 ...  \n",
       "522  [{'@CWE_ID': '300'}, {'@CWE_ID': '290'}, {'@CW...  \n",
       "523                                 {'@CWE_ID': '538'}  \n",
       "524           [{'@CWE_ID': '589'}, {'@CWE_ID': '227'}]  \n",
       "525  [{'@CWE_ID': '327'}, {'@CWE_ID': '1240'}, {'@C...  \n",
       "526                                               None  \n",
       "\n",
       "[527 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAPEC_df = pd.DataFrame(dict)\n",
    "CAPEC_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "racial-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus from CAPEC descriptions\n",
    "corpus = ''\n",
    "\n",
    "for desc in Desc:\n",
    "    try:\n",
    "        corpus += desc +'\\n'\n",
    "    except:\n",
    "        if desc:\n",
    "            corpus += desc['html:p'][0] +'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exciting-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = nlp(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corrected-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/CAPEC_corpus.txt\", \"w\") as text_file:\n",
    "    text_file.write(corpus)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-turning",
   "metadata": {},
   "source": [
    "# Co-occurrence Matrix using Autophrase extracted phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-bouquet",
   "metadata": {},
   "source": [
    "## Load Autphrase results\n",
    "The Autophraser is executed separately using the CAPEC_corpus.txt file renamed to DBLP.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "harmful-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/AutoPhrase_multi-words.txt') as f:\n",
    "    lines = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "lonely-pastor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9731306755"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(lines[0].split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "material-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "phrases_list = []\n",
    "for line in lines:\n",
    "    score = float(line.split('\\t')[0])\n",
    "    if score >= threshold:\n",
    "        phrases_list.append(line.split('\\t')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "public-paint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sql injection',\n",
       " 'ip address',\n",
       " 'cross site scripting',\n",
       " 'social engineering',\n",
       " 'web browser',\n",
       " 'buffer overflow',\n",
       " 'web server',\n",
       " 'operating systems',\n",
       " 'supply chain',\n",
       " 'sequence number']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print example phrases\n",
    "phrases_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-importance",
   "metadata": {},
   "source": [
    "## Get descriptions, tokenize, make lower-case, and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "challenging-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pretty-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply on the \"Desc\" list of descriptions instead of the aggregate corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-offense",
   "metadata": {},
   "source": [
    "## Tokenize descriptions and remove punctuation. Combine into list of single strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "checked-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_descriptions = []\n",
    "for desc in Desc:\n",
    "    if isinstance(desc,str):\n",
    "        description = desc\n",
    "    elif desc:\n",
    "        description = desc['html:p'][0]\n",
    "    word_tokens = word_tokenize(description.lower())\n",
    "    temp_list = []\n",
    "    for w in word_tokens:  \n",
    "        if w.isalpha():\n",
    "            temp_list.append(w)  \n",
    "    filtered_descriptions.append(\" \".join(temp_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-kentucky",
   "metadata": {},
   "source": [
    "## Tokenize corpus, and remove punctuation (Not currently used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "identified-strain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#stop_words = set(stopwords.words('english'))  \\n\\nword_tokens = word_tokenize(corpus.lower())\\n  \\n#filtered_corpus = [w for w in word_tokens if not w in stop_words]  \\n  \\nfiltered_corpus = []  \\n  \\nfor w in word_tokens:  \\n#    if w not in stop_words and w.isalpha():  \\n     if w.isalpha():\\n        filtered_corpus.append(w)  \\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "word_tokens = word_tokenize(corpus.lower())\n",
    "  \n",
    "#filtered_corpus = [w for w in word_tokens if not w in stop_words]  \n",
    "  \n",
    "filtered_corpus = []  \n",
    "  \n",
    "for w in word_tokens:  \n",
    "#    if w not in stop_words and w.isalpha():  \n",
    "     if w.isalpha():\n",
    "        filtered_corpus.append(w)  \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-reduction",
   "metadata": {},
   "source": [
    "## Iterate and find Autophrase phrases in the corpus that match exactly\n",
    "This function returns the start and end index of the corpus list of words where the phrase matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "auburn-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_matches(split_phrase, filered_corpus):\n",
    "    # Find index of all matches of first word in the phrase\n",
    "    num_words = len(split_phrase)\n",
    "    indices = [i for i, x in enumerate(filtered_corpus) if x == split_phrase[0]]\n",
    "    results = []\n",
    "    # Iterate over all first word match indices\n",
    "    for index_num in indices:\n",
    "        # Iterate over all words in phrase\n",
    "        all_sum = 0\n",
    "        for i in range(num_words):\n",
    "            try: \n",
    "                if split_phrase[i]==filtered_corpus[index_num+i]:\n",
    "                    all_sum += 1\n",
    "                else:\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "        if all_sum == num_words:\n",
    "            results.append((index_num, index_num + num_words - 1))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-cause",
   "metadata": {},
   "source": [
    "This function checks if any phrase exists in the N words surrounding the first phrase found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "romance-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_match(split_phrase, words, main_phrase):\n",
    "    # Find index of all matches of first word in the phrase\n",
    "    num_words = len(split_phrase)\n",
    "    indices = [i for i, x in enumerate(words) if x == split_phrase[0]]\n",
    "    results = []\n",
    "    # Iterate over all first word match indices\n",
    "    for index_num in indices:\n",
    "        # Iterate over all words in phrase\n",
    "        all_sum = 0\n",
    "        for i in range(num_words):\n",
    "            try: \n",
    "                if split_phrase[i]==words[index_num+i]:\n",
    "                    all_sum += 1\n",
    "                else:\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "        if all_sum == num_words:\n",
    "            return((' '.join(split_phrase), main_phrase))\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-merchandise",
   "metadata": {},
   "source": [
    "## Check if any phrase exists in the N previous and N after words (N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "above-marketplace",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-9ea31cac6859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmatches_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmain_phrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphrases_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mindex_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_all_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_phrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiltered_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mwords_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mindex_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "N = 9999\n",
    "matches_list = []\n",
    "for main_phrase in phrases_list:\n",
    "    index_pairs = find_all_matches(main_phrase.split(' '),filtered_corpus)\n",
    "    for index_pair in index_pairs:\n",
    "        words_prev = filtered_corpus[index_pair[0] - N : index_pair[0]]\n",
    "        words_after = filtered_corpus[index_pair[1] + 1 : index_pair[1] + N + 1]\n",
    "        for phrase in phrases_list:\n",
    "            # Check before\n",
    "            pair_prev = check_match(phrase.split(' '),words_prev, main_phrase)\n",
    "            pair_after = check_match(phrase.split(' '),words_after, main_phrase)\n",
    "            if pair_prev != 0:\n",
    "                matches_list.append(pair_prev)\n",
    "            if pair_after != 0:\n",
    "                matches_list.append(pair_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example phrase matches')\n",
    "matches_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-fashion",
   "metadata": {},
   "source": [
    "## Make co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_co_occurrence_matrix(phrases_list,matches_list):\n",
    "    vocab = set(phrases_list) \n",
    "    vocab = list(vocab)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    # Create bigrams from all words in corpus\n",
    "    bi_grams = matches_list\n",
    "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    # Initialize co-occurrence matrix\n",
    "    # co_occurrence_matrix[current][previous]\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # Loop through the bigrams taking the current and previous word,\n",
    "    # and the number of occurrences of the bigram.\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_index[current]\n",
    "        pos_previous = vocab_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous] = count\n",
    "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
    "    # return the matrix and the index\n",
    "    return co_occurrence_matrix, vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocab_index = generate_co_occurrence_matrix(phrases_list,matches_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = pd.DataFrame(matrix, index=vocab_index,\n",
    "                             columns=vocab_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-sigma",
   "metadata": {},
   "source": [
    "## Sort by the sum of the all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort rows of data_matrix by the sum of the columns of each row in descending order\n",
    "data_matrix['sum_column'] = data_matrix.sum(axis=1)\n",
    "data_matrix = data_matrix.sort_values(by='sum_column', ascending=False)\n",
    "data_matrix = data_matrix.drop(columns=['sum_column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-antigua",
   "metadata": {},
   "source": [
    "## Sort by the sum of all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix.loc['sum_row'] = data_matrix.sum(axis=1)\n",
    "data_matrix = data_matrix.sort_values(by = 'sum_row',ascending=False, axis=1)\n",
    "data_matrix = data_matrix.drop(index='sum_row')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-garden",
   "metadata": {},
   "source": [
    "## Display Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_matrix.iloc[0:100,0:100]\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.heatmap(df, annot=False, cmap=\"rocket_r\")\n",
    "#plt.figure(figsize=(25,25))\n",
    "#plt.imshow(df)\n",
    "#plt.colorbar()\n",
    "#plt.xticks(range(len(df)),df.columns, rotation=90)\n",
    "#plt.yticks(range(len(df)),df.index)\n",
    "#plt.ylabel('Frequency')\n",
    "plt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid()\n",
    "plt.savefig('CAPEC_matrix.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-acting",
   "metadata": {},
   "source": [
    "## Save Sorted Data Matrix to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-papua",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_matrix.to_csv('data/CAPEC_coocurrence_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-official",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
