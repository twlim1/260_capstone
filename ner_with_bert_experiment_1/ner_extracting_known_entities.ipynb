{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "middle-power",
   "metadata": {},
   "source": [
    "# NER BERT Experiment 1\n",
    "This code will install and run all the required packages needed to run this note book.  \n",
    "It is recommended to use a venv to not compromise system packages.  \n",
    "\n",
    "This code will extract phrases that could be considered vulnerability phrases in NVD dataset.\n",
    "The code will process and prepare the phrases to be used for BERT fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wheel \n",
    "!pip install setuptools\n",
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision\n",
    "!pip install wget\n",
    "!pip install tensorflow\n",
    "!pip install spacy\n",
    "!python3 -m spacy download en_core_web_lg\n",
    "!git clone https://github.com/huggingface/transformers.git\n",
    "!cd transformers; pip install -e .;\n",
    "!cp custom_run.sh transformers/examples/legacy/token-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(string.punctuation)\n",
    "# s.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "NVD_DATA = ['https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2021.json.gz',\n",
    "           'https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2020.json.gz']\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "import wget\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Download the file (if we haven't already)\n",
    "for url in NVD_DATA:\n",
    "    f_name_comp = url.split('/')[-1]\n",
    "    f_name_decomp = url.split('/')[-1].strip('.gz')\n",
    "  \n",
    "    if not os.path.exists(f_name_comp):\n",
    "        wget.download(url, f_name_comp)\n",
    "\n",
    "    if not os.path.exists(f_name_decomp):\n",
    "        with gzip.open(f_name_comp, 'rb') as f_in:\n",
    "            with open(f_name_decomp, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "                \n",
    "# Import description of CVEs from nvd json files\n",
    "cve_dict_list = []\n",
    "for url in NVD_DATA:\n",
    "    f_name_decomp = url.split('/')[-1].strip('.gz')\n",
    "    with open(f_name_decomp) as f:\n",
    "        cve_dict_list.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ''\n",
    "counter = 0\n",
    "LIMIT = 5000\n",
    "\n",
    "sentence_set = set()\n",
    "SENT_FLAG = 'SENTSTARTFLAG'\n",
    "\n",
    "for cve_dict in cve_dict_list:\n",
    "    for cve in cve_dict['CVE_Items']:\n",
    "        desc_list = cve['cve']['description']['description_data']\n",
    "        for desc in desc_list:\n",
    "            sentences = [x.lower() for x in desc['value'].split('. ')]\n",
    "            for sentence in sentences:\n",
    "                if sentence in sentence_set:\n",
    "                    continue\n",
    "                \n",
    "                s = sentence.translate(str.maketrans(' ', ' ', '''!\"#$%&'()*+,/:;<=>?@[\\]^_`{|}~'''))\n",
    "                sentence_set.add(s)\n",
    "                corpus += f'{SENT_FLAG} {s}\\n'\n",
    "\n",
    "                counter += 1\n",
    "                if counter >= LIMIT:\n",
    "                    break\n",
    "            if counter >= LIMIT:\n",
    "                break\n",
    "        if counter >= LIMIT:\n",
    "            break\n",
    "    if counter >= LIMIT:\n",
    "        break\n",
    "\n",
    "print(len(sentence_set))\n",
    "# sentence_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [['DET', 'ADJ', 'NOUN', 'NOUN', 'NOUN'],\n",
    "['DET', 'NOUN', 'PROPN', 'PROPN', 'NOUN'],\n",
    "['DET', 'ADJ', 'NOUN', 'NOUN', 'NOUN'],\n",
    "['DET', 'PROPN', 'NOUN', 'NOUN'],\n",
    "['DET', 'PROPN', 'NOUN', 'NOUN'],\n",
    "['DET', 'NOUN', 'NOUN', 'NOUN'],\n",
    "['DET', 'ADJ', 'NOUN', 'NOUN', 'NOUN'],\n",
    "['DET', 'NOUN', 'NOUN', 'VERB', 'NOUN'],\n",
    "['DET', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN'],\n",
    "['DET', 'ADJ', 'NOUN', 'NOUN', 'NOUN'],\n",
    "['DET', 'ADJ', 'ADJ', 'NOUN', 'NOUN'],\n",
    "['DET', 'NOUN', 'PUNCT', 'VERB', 'NOUN', 'VERB', 'NOUN'],\n",
    "['DET', 'VERB', 'ADJ', 'ADJ', 'ADJ', 'NOUN', 'NOUN'],\n",
    "['DET', 'ADJ', 'PROPN', 'NOUN', 'NOUN'],\n",
    "['DET', 'NOUN', 'PUNCT', 'ADP', 'PUNCT', 'ADJ', 'NOUN']]\n",
    "\n",
    "s = set()\n",
    "for i in a:\n",
    "    for j in i:\n",
    "        s.add(j)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.max_length = 10000000\n",
    "doc = nlp(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_black_list(w_list):\n",
    "    BLK_LIST = [\n",
    "        ['exploitable', 'vulnerability'], \n",
    "        ['exploit', 'vulnerability'],\n",
    "        ['vulnerability']]\n",
    "    \n",
    "    lower_case_list = [x.lower() for x in w_list]\n",
    "    for unwanted in BLK_LIST:\n",
    "        if lower_case_list == unwanted:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_vul_phrase(w_qu, p_qu):\n",
    "    rev_w_qu = reversed(w_qu)\n",
    "    rev_p_qu = reversed(p_qu)\n",
    "    acceptable_pos = {'ADP', 'DET', 'NOUN', 'PUNCT', 'ADJ', 'PROPN', 'VERB'}\n",
    "    \n",
    "    w_result = []\n",
    "    p_result = []\n",
    "    \n",
    "   \n",
    "    for w, p in zip(rev_w_qu, rev_p_qu):\n",
    "        if p in acceptable_pos:\n",
    "            w_result.append(w)\n",
    "            p_result.append(p)\n",
    "        else:\n",
    "            if is_in_black_list(reversed(w_result)):\n",
    "                return False, False\n",
    "            else:\n",
    "                return list(reversed(w_result)), list(reversed(p_result))\n",
    "        \n",
    "        if p_result == ['NOUN', 'DET']:\n",
    "            return False, False\n",
    "    \n",
    "        if p_result == ['NOUN', 'ADP']:\n",
    "            return False, False\n",
    "        \n",
    "        if p == 'DET' and len(p_result) > 2:\n",
    "            break       \n",
    "\n",
    "    if is_in_black_list(reversed(w_result)):\n",
    "        return False, False\n",
    "    \n",
    "    if p_result[-1] == 'PUNCT':\n",
    "        w_result = w_result[:-1]\n",
    "        p_result = p_result[:-1]\n",
    "    \n",
    "    if p_result[-1] == 'DET':\n",
    "        w_result = w_result[:-1]\n",
    "        p_result = p_result[:-1]\n",
    "    \n",
    "    return list(reversed(w_result)), list(reversed(p_result))\n",
    "\n",
    "def update_ner_tag(w_list, v_list):\n",
    "    last_index = len(v_list) - 1\n",
    "    rev_w_list = reversed(w_list)\n",
    "    for i, w in enumerate(rev_w_list):\n",
    "        if v_list[last_index - i]['tok'] != w:\n",
    "            raise ValueError\n",
    "        if i == (len(w_list) - 1):\n",
    "            v_list[last_index - i]['tag'] = 'B-VUL'\n",
    "        else:\n",
    "            v_list[last_index - i]['tag'] = 'I-VUL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "wrd_queue = queue.Queue(20) \n",
    "pos_queue = queue.Queue(20) \n",
    "\n",
    "vocab_pos_ner_list = []\n",
    "\n",
    "FLAG_WORD = 'vulnerability'\n",
    "\n",
    "extracted_phrases = set()\n",
    "sent_length = []\n",
    "for sent in doc.sents:\n",
    "    sent_length.append(len(list(sent)))\n",
    "    \n",
    "    for tok in sent:\n",
    "        if wrd_queue.full():\n",
    "            _ = wrd_queue.get()\n",
    "            _ = pos_queue.get()\n",
    "        \n",
    "        tk = tok.text\n",
    "        tp = tok.pos_\n",
    "#         if tok.text == SENT_FLAG:\n",
    "#             tk = '-DOCSTART-'\n",
    "#             tp = '-DOCSTART-'\n",
    "            \n",
    "        vocab = {\n",
    "            'tok': tk,\n",
    "            'pos': tp,\n",
    "            'tag': 'O'}\n",
    "            \n",
    "        vocab_pos_ner_list.append(vocab)\n",
    "        wrd_queue.put_nowait(tk)\n",
    "        pos_queue.put_nowait(tp)\n",
    "        if tok.text.lower() == FLAG_WORD:\n",
    "            w, p = get_vul_phrase(list(wrd_queue.queue), \n",
    "                                  list(pos_queue.queue))\n",
    "            if w:\n",
    "                update_ner_tag(w, vocab_pos_ner_list)\n",
    "                extracted_phrases.add(' '.join(list(w)).strip(SENT_FLAG).strip())\n",
    "#                 print(list(w))\n",
    "#                 print(list(p))\n",
    "#                 print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(extracted_phrases))\n",
    "extracted_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sent_length.sort()\n",
    "pd.Series(sent_length).hist(bins=50, figsize=(8,5))\n",
    "# plt.xticks(np.linspace(0,1000,50));\n",
    "plt.xticks(rotation=90);\n",
    "print(pd.Series(sent_length).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vocab_pos_ner_list)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['tok'] == SENT_FLAG]\n",
    "df.to_csv('vul_tags.csv', index=False)\n",
    "train_perc = int(df.shape[0] * 50/100)\n",
    "dev_perc = train_perc + int(df.shape[0] * 25/100)\n",
    "\n",
    "df.loc[0:train_perc].to_csv('train.txt.tmp', sep=' ', columns=['tok', 'tag'], index=False, header=False)\n",
    "df.loc[train_perc: dev_perc].to_csv('dev.txt.tmp', sep=' ', columns=['tok', 'tag'], index=False, header=False) #, header=['-DOCSTART-', 'O'])\n",
    "df.loc[dev_perc:].to_csv('test.txt.tmp', sep=' ', columns=['tok', 'tag'], index=False, header=False) #header=['-DOCSTART-', 'O'])\n",
    "\n",
    "labels = df['tag'].unique()\n",
    "with open('labels.txt', 'w') as f:\n",
    "    for l in labels:\n",
    "        f.write(f'{l}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp train.txt.tmp transformers/examples/legacy/token-classification/\n",
    "!cp dev.txt.tmp transformers/examples/legacy/token-classification/\n",
    "!cp test.txt.tmp transformers/examples/legacy/token-classification/\n",
    "!cp labels.txt transformers/examples/legacy/token-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-gather",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd transformers/examples/legacy/token-classification; ./custom_run.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
